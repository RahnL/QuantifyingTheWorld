---
title: "R Notebook"
output: html_notebook
---

Based on Eric Larons jupyter notebook, and code from the book we're using (add references!)

Download and parse data
```{python}
import os, sys
path = "Data"
if not os.path.exists(path):
    os.mkdir( path, 0755)
    
import urllib # this is part of the standard library for python

years_to_download = range(1987,2009) # get the years 1987 through 2008
baseurl = 'http://stat-computing.org/dataexpo/2009/%d.csv.bz2' 

files = []
for year in years_to_download:
    # prepare strings
    url_of_data_file = baseurl%(year) # get the URL for the data file
    save_as_filename = 'Data/%d.csv.bz2'%(year) # save as this
    files += [save_as_filename] # save name of the compressed file
    
    if not os.path.isfile(save_as_filename):
        # download file if it doesn't exist
        print 'Downloading %s to %s'%(url_of_data_file, save_as_filename) # progress update
        urllib.urlretrieve(url_of_data_file, save_as_filename) #execute download
    else:
        print 'File %s already exists. downloading skipped'%save_as_filename
        
    
print files  

import bz2 # this is also part of the python standard library

# Now lets decompress all the files
for filename in files:
    # get file names
    filepath = filename
    newfilepath = filename[:-4]
    if not os.path.isfile(newfilepath):
        print 'Decompressing', filepath,'to', newfilepath
    
        # go through the decompressed chunks and write out to a decompressed file
        with open(newfilepath, 'wb') as new_file, bz2.BZ2File(filepath, 'rb') as file:
            for data in iter(lambda : file.read(100 * 1024), b''):
                new_file.write(data)
    else:
        print filepath,'already is uncompressed.'
        
```



## 2 - Load files into memory
```{python}
import pandas as pd
import numpy as np
import sys

total_length=0
for year in [1987, 1988]:
    csvfile = 'Data/%d.csv'%(year)
    print 'loading', csvfile
    sys.stdout.flush()
    
    df=pd.read_csv(csvfile)
    
    total_length+=len(df)
    
print 'Answer from python', total_length
df.head()
```


Do same import as above python, but in R, for comparison
```{r}
# need this, since working directory is different between python code and r
setwd("C:/Users/rahnl/Documents/Data")

total_length <- 0
for(year in 1987:1988){
    filename = paste(year,".csv",sep="")
    x<-read.csv(filename)
    total_length <- total_length + nrow(x)
}
print(total_length)
```

## 2.1 - data preprocessing
```{python}
# In this data, there are multiple variables that are objects
# these need to be appropriately converted to numbers (integers) 
# To do this, we must know the uniqe values in each of the columns, let's do this first
import pandas as pd
import numpy as np
import sys
import time
import cPickle as pickle

unique_values = {} # create an empty dictionary of the column name an the unique values in it
for year in range(1987,2009):
    t = time.time()
    # get file name of the csv
    csvfile = 'Data/%d.csv'%(year)
    print 'loading',csvfile,
    sys.stdout.flush()
    
    # read the file
    df = pd.read_csv(csvfile,usecols=['Origin', 'Dest', 'UniqueCarrier','TailNum','CancellationCode']) 
    #df = df.select_dtypes(exclude=['float64','int64']) # grab only the non-numeric data
    
    print '...finding unique values',
    sys.stdout.flush()
    
    for col in df.columns:
        # check to see if we have seen this column before
        s = set(df[col].values.astype(np.str))
        if col not in unique_values:
            # if not, then create a key with the unique values for that column in it
            unique_values[col] = s
        else:
            # otherwise make sure that the remaining columns are unique
            unique_values[col] |= s
            
    print '...finished, %.2f seconds'%(time.time()-t)
    sys.stdout.flush()
    del df

# Save out the dictionary for later use
pickle.dump( unique_values, open( "Data/unique_mapping.p", "wb" ) )
```

