{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loading Data/1987.csv.bz2 ...finding unique values ...finished, 10.94 seconds\n",
      "loading Data/1988.csv.bz2"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c2acf2561b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[1;33m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Origin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Dest'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UniqueCarrier'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'TailNum'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CancellationCode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'CancellationCode'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     ,na_values=['?'])\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[1;31m#'TailNum': str,'Cancelled':int, 'Origin': str, #'Dest':str,'UniqueCarrier':str,'TailNum':str,'CancellationCode':str})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[1;31m#df = df.select_dtypes(exclude=['float64','int64']) # grab only the non-numeric data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m _parser_defaults = {\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skip_footer not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# In this data, there are multiple variables that are objects\n",
    "# these need to be appropriately converted to numbers (integers) \n",
    "# To do this, we must know the uniqe values in each of the columns, let's do this first\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import cPickle as pickle\n",
    "\n",
    "unique_values = {} # create an empty dictionary of the column name an the unique values in it\n",
    "for year in range(1987,2009):\n",
    "    t = time.time()\n",
    "    # get file name of the csv\n",
    "    csvfile = 'Data/%d.csv.bz2'%(year)\n",
    "    print 'loading',csvfile,\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # read the file\n",
    "    df = pd.read_csv(csvfile\n",
    "                    ,usecols=['Origin', 'Dest', 'UniqueCarrier','TailNum','CancellationCode']\n",
    "                    ,dtype={'CancellationCode':str}\n",
    "                    ,na_values=['?'])\n",
    "        #'TailNum': str,'Cancelled':int, 'Origin': str, #'Dest':str,'UniqueCarrier':str,'TailNum':str,'CancellationCode':str}) \n",
    "    #df = df.select_dtypes(exclude=['float64','int64']) # grab only the non-numeric data\n",
    "   \n",
    "    print '...finding unique values',\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # check to see if we have seen this column before\n",
    "        s = set(df[col].values.astype(np.str))\n",
    "        if col not in unique_values:\n",
    "            # if not, then create a key with the unique values for that column in it\n",
    "            unique_values[col] = s\n",
    "        else:\n",
    "            # otherwise make sure that the remaining columns are unique\n",
    "            unique_values[col] |= s\n",
    "            \n",
    "    print '...finished, %.2f seconds'%(time.time()-t)\n",
    "    sys.stdout.flush()\n",
    "    del df\n",
    "\n",
    "# Save out the dictionary for later use\n",
    "pickle.dump( unique_values, open( \"Data/unique_mapping.p\", \"wb\" ) )\n",
    "\n",
    "print unique_values.keys()\n",
    "print 'One example:',unique_values['CancellationCode']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def fast_numpy_replace(np_vector,replace_set):\n",
    "    # you can look at this function at your leisure, but essentially we use fast set \n",
    "    # comparison to try and speed up the analysis\n",
    "    replace_set = np.array(list(replace_set)) # get \"possible values\" as a numpy array\n",
    "    n = np.ndarray(np_vector.shape).astype(np.float64) # fill in this matrix\n",
    "    \n",
    "    vector_as_set,idx_back = np.unique(np_vector,return_inverse=True) # get the unique indices and locations\n",
    "    \n",
    "    # now loop through the unique values for this dataset\n",
    "    for idx,val in enumerate(vector_as_set):\n",
    "        # find what number this should be (like a hash)\n",
    "        category_num = np.nonzero(replace_set == val)[0][0]\n",
    "        n[idx_back==idx] = category_num # set the values as this category, vectorize for speed\n",
    "        \n",
    "    return n.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running... Data/1987.csv.gz2"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "File Data/1987.csv.gz2 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bc2db410f9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[1;31m# read the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'loaded, ...replacing values'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rahnl\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File Data/1987.csv.gz2 does not exist"
     ]
    }
   ],
   "source": [
    "fileHandle = open('Data/AirlineDataAll.csv', 'w') # open and replace if needed\n",
    "years = range(1987,2009)\n",
    "for year in years:\n",
    "    t = time.time()\n",
    "    \n",
    "    # get file name of the csv\n",
    "    csvfile = 'Data/%d.csv'%(year)\n",
    "    print 'Running...',csvfile,\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # read the file\n",
    "    df = pd.read_csv(csvfile, error_bad_lines=False) \n",
    "    \n",
    "    print 'loaded, ...replacing values',sys.stdout.flush()\n",
    "        \n",
    "    # now replace the matching columnar data with the proper number category\n",
    "    for key in unique_values.keys():\n",
    "        if key in df:\n",
    "            print key[0:4],\n",
    "            sys.stdout.flush()\n",
    "            tmp = df[key].values.astype(np.str)\n",
    "            df[key] = fast_numpy_replace(tmp,unique_values[key])\n",
    "     \n",
    "    print '...',sys.stdout.flush()\n",
    "    \n",
    "    for col in df:\n",
    "        df[col] = np.round(df[col].astype(np.float64)) # use floats to keep the nan's inline with numpy representation\n",
    "    \n",
    "    print 'writing',sys.stdout.flush()\n",
    "    \n",
    "    # these lines make one large file with the numeric data\n",
    "    # it also solves a problem with pandas closing the file that takes an inordinate amount of time\n",
    "    # NOTE: using binary here would be a huge speedup, but I am not sure about the binary structure of the \n",
    "    # backing file for bigmatrix, so we stick with CSV\n",
    "    # TODO: find out if the backing file is just a dump of the c struct to file\n",
    "    if year==years[0]:\n",
    "        df.to_csv(fileHandle,index=False, index_label=False, na_rep=\"NA\",float_format='%.0f')\n",
    "    else:\n",
    "        df.to_csv(fileHandle, mode='a', header=False, index=False, index_label=False,  na_rep=\"NA\", float_format='%.0f')\n",
    "        \n",
    "    print ', %.2f sec.'%(time.time()-t)\n",
    "    del df\n",
    "\n",
    "print 'closing file',\n",
    "sys.stdout.flush()\n",
    "\n",
    "fileHandle.close()\n",
    "print '...Done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: u'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep -m 10 \"@@\" data/2001.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# now lets take a look to see what has actually changed in the file\n",
    "# let's load the head of 1987 and the big CSV file to see how they compare\n",
    "print 'New File Format:'\n",
    "!head Data/AirlineDataAll.csv\n",
    "print ''\n",
    "print 'Old File Format'\n",
    "!head Data/1987.csv\n",
    "\n",
    "\n",
    "# let's now look at the tail of our big dataset and the tail of the 2008 file\n",
    "# do they compare nicely?\n",
    "print 'New File Format:'\n",
    "!tail Data/AirlineDataAll.csv\n",
    "print ''\n",
    "print 'Old File Format:'\n",
    "!tail Data/2008.csv\n",
    "\n",
    "\n",
    "!ls -all Data/*All.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4 Handling out of core memory and analyzing data using Graphlab Create\n",
    "# 4.1 Loading 12 gb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "sf = gl.SFrame('Data/AirlineDataAll.csv')\n",
    "sf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Preprocess: Concatenate and save compressed binary version to perform operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del sf # get rid of the old thing\n",
    "# What about just loading up all the data using the SFrame Utility for loading CSV files?\n",
    "# We will need to make sure that the SFrame has consistent datatypes, so we will give the value for each header\n",
    "column_hints=[int,int,int,int,int,int,int,int,str,int,str,int,int,int,int,int,str,str,int,int,int,int,str,int,int,int,int,int,int]\n",
    "\n",
    "t = time.time()\n",
    "# now load the first SFrame\n",
    "sf = gl.SFrame() #.read_csv('Data/1987.csv',column_type_hints=column_hints)\n",
    "\n",
    "# and then append each SFrame in a for loop\n",
    "for year in range(1987,2009):\n",
    "    print 'read %d lines, reading next file %d.csv'%(sf.shape[0],year)\n",
    "    sys.stdout.flush()\n",
    "    sftmp = gl.SFrame.read_csv('Data/%d.csv'%(year),column_type_hints=column_hints)\n",
    "    sf = sf.append(sftmp)\n",
    "\n",
    "print 'It took %.2f seconds to concatenate the memory mapped file'%(time.time()-t)\n",
    "\n",
    "t = time.time()\n",
    "print 'Saving...',\n",
    "sf.save('Data/sframe_directory') # save a compressed version of this SFrame\n",
    "print 'took %.2f seconds'%(time.time()-t),'Shape of SFrame is',sf.shape\n",
    "\n",
    "\n",
    "\n",
    "# If you have already run the notebook above and just want to load up the data\n",
    "# then you can reload the SFrame here\n",
    "import graphlab as gl\n",
    "\n",
    "sf = gl.load_sframe('Data/sframe_directory')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Analyzing popular airports to fly from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to perform grouping and splitting\n",
    "# we need to specify (1) which column(s) to group the SFrame using, and \n",
    "#                    (2) what function we want to perform on the group\n",
    "# in graphlab, we only have a few options for performing on each of the groups. \n",
    "# Here, lets keep it simple--let's group by the airport origin and then\n",
    "#  use the builtin 'count' function to aggregate the results\n",
    "# The result is another SFrame with the Unique origin names as a column and the\n",
    "#  number of entries in each group in another column\n",
    "%time sf_counts = sf.groupby('Origin', {'num_flights':gl.aggregate.COUNT()})\n",
    "sf_counts\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# As seen above, the sf_counts SFrame has the origin of the flight on the left\n",
    "# and the count of flights on the right \n",
    "\n",
    "# let's grab the top 10 entries\n",
    "sf_top = sf_counts.topk('num_flights',10) # this is builtin command in graphlab\n",
    "\n",
    "airports = np.array(sf_top['Origin'])\n",
    "counts = np.array(sf_top['num_flights'])\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "plt.barh(range(len(counts)),counts)\n",
    "\n",
    "# and set them on the plot\n",
    "plt.yticks(range(len(airports)), airports)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Analyzing Departure delays at specific times of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "```{python}\n",
    "from math import floor\n",
    "# first, let's create a new column in this SFrame that has the departure time floored to the nearest hour\n",
    "sf['DepTimeByHour'] = sf['CRSDepTime'].apply(lambda x: floor(x/100),dtype=int)\n",
    "sf['DepTimeByHour'] # and print a few of them (note: the column has not been evaluated yet)\n",
    "\n",
    "# Let's now change the hours of the day that are equal to 24\n",
    "# we need to be careful here becasue each column is not immutable\n",
    "# in pandas this would be:\n",
    "#    df.DepTimeByHour[df.DepTimeByHour==24] = 0\n",
    "# but we cant just change a few values in the column, we need to change them all and replace them\n",
    "# don't worry though, Graphlab does this smartly\n",
    "sf['DepTimeByHour'] = sf['DepTimeByHour'].apply(lambda x: 0 if x==24 else x)\n",
    "# again, this column has not been evaluated yet because that value has not yet been accessed\n",
    "\n",
    "# now lets group the SFrame by the hours and calculate the percentiles of each group\n",
    "# here is where the lazy evaluation actually happens so this takes a little while to compute\n",
    "\n",
    "# the groupby function will partition our SFrame into groups based upon the given column of data\n",
    "# next, we need to tell graphlab what operations to perform on the group and what rows\n",
    "# to do that, we send in a dictionary of names and 'operations' \n",
    "# We did a similar operation above with the 'COUNT' aggregator\n",
    "# there are only a certain number of operators we can choose from, we will choose to use the 'QUANTILE'\n",
    "#   aggregator on the column 'DepDelay'. We want to take the percentiles [0.90,0.99,0.999,0.9999]\n",
    "# We can also perform other operations by adding entries in the dictonary\n",
    "# So we will also take the 'MAX' of each group\n",
    "import time\n",
    "t = time.time()\n",
    "delay_sf = sf.groupby('DepTimeByHour', \n",
    "                            {'delay_quantiles':gl.aggregate.QUANTILE('DepDelay', [0.90,0.99,0.999,0.9999]),\n",
    "                             'delay_max':gl.aggregate.MAX('DepDelay')})\n",
    "# this returns a new SFrame with the specified columns from each aggregation\n",
    "\n",
    "print 'Took %.2f seconds to run'%(time.time()-t)\n",
    "\n",
    "\n",
    "# sort it by when departed and display it\n",
    "delay_sf = delay_sf.sort('DepTimeByHour')\n",
    "delay_sf\n",
    "\n",
    "\n",
    "\n",
    "# to use matplotlib, we need to convert over to numpy arrays\n",
    "# this is a fine operation because the new aggregated SFrame we are \n",
    "# working (delay_sf) with is quite small\n",
    "x = np.array(delay_sf['DepTimeByHour'])\n",
    "y = np.array(delay_sf['delay_quantiles'])\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('Minutes Delayed')\n",
    "plt.xlabel('Hour of Day')\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylim(0,1400) # make the same axes as in the book\n",
    "plt.legend(['0.9','0.99','0.999','0.9999'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only use years where the tail number was recorded\n",
    "# we can manipulate the SFrame fairly easily in graphlab, so let's do it\n",
    "sf_tmp = sf[['TailNum','Year','Month','DepDelay']][sf['Year']>1994]\n",
    "# lets try to make a function for getting the age of the plane\n",
    "# First lets just save the plane's age in years\n",
    "sf_tmp['FlightAge'] = 12*sf_tmp['Year']+sf_tmp['Month']-1\n",
    "\n",
    "# and take the minimum of that in order to get its first flight\n",
    "t = time.time()\n",
    "sf_min_ages = sf_tmp[['TailNum','FlightAge']].groupby('TailNum',{'FirstFlight':gl.aggregate.MIN('FlightAge')})\n",
    "print 'Took %.2f seconds to run'%(time.time()-t)\n",
    "# Now transform the FirstFlight Column into the original dataframe size\n",
    "# to do that we can just do a join on a few columns of our sf\n",
    "# this will save the flight age and the minimum in a new SFrame\n",
    "%time sf_fewcols = sf_tmp[['TailNum','FlightAge']].join(sf_min_ages,on='TailNum',how='left') # long operation\n",
    "# and now we can simply subtract the new calculated quantity and add to the original SFrame\n",
    "sf_tmp['Age'] = sf_fewcols['FlightAge']-sf_fewcols['FirstFlight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now look at the age and delay time in terms of regression (like your book)\n",
    "%time lin_model = gl.linear_regression.create(sf_tmp['DepDelay','Age'].dropna(), target='DepDelay', features=['Age'])\n",
    "\n",
    "lin_model['coefficients']"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
